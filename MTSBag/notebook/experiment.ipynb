{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.34it/s]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "100%|██████████| 10/10 [03:34<00:00, 21.49s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_experiment = 10\n",
    "data_list = ['wine', 'abalone','letter']\n",
    "\n",
    "for data in data_list:\n",
    "    MTS_result_df = pd.DataFrame(\n",
    "                        columns=['AUC', 'accuracy', 'recall', 'precision', 'gmeans', 'RS'],\n",
    "                        index=range(n_experiment))\n",
    "    for m in tqdm(range(n_experiment)):\n",
    "        \n",
    "        select_data = data\n",
    "\n",
    "        if select_data == 'letter':\n",
    "            # データの取得\n",
    "            df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "            # Aのみを判定するため，Aを0，A以外を1にした．\n",
    "            # 少数派のAを正常，その他を異常データと定義\n",
    "            df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "            #Xとyを入力\n",
    "            X = df[range(1,17)]\n",
    "            y = df[0]\n",
    "\n",
    "        elif select_data == 'wine':\n",
    "\n",
    "            import tensorflow as tf\n",
    "\n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('wine.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            column_names = ['Alcohol',\n",
    "            'Malic acid',\n",
    "            'Ash',\n",
    "            'Alcalinity of ash',\n",
    "            'Magnesium',\n",
    "            'Total phenols',\n",
    "            'Flavanoids',\n",
    "            'Nonflavanoid phenols',\n",
    "            'Proanthocyanins',\n",
    "            'Color intensity',\n",
    "            'Hue',\n",
    "            'OD280/OD315 of diluted wines',\n",
    "            'Proline' \n",
    "            ]\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=column_names)\n",
    "            raw_data['y'] = raw_data.index\n",
    "            raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "            raw_data['y'] = raw_data['y'].apply(lambda x: 0 if x == 3 else 1)\n",
    "\n",
    "            X = raw_data.drop('y', axis=1)\n",
    "            y = raw_data['y']\n",
    "        \n",
    "        elif select_data == 'abalone':\n",
    "\n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('abalone.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=range(8)).reset_index(drop=True)\n",
    "\n",
    "            raw_data[7] = raw_data[7].apply(lambda x: 1 if x > 4 else 0)\n",
    "\n",
    "\n",
    "            X = raw_data.drop(7, axis=1)\n",
    "            y = raw_data[7]\n",
    "\n",
    "        else:\n",
    "            print('そのデータはありません')\n",
    "\n",
    "        # バギング側の話\n",
    "        # ブートストラップサンプリングの個数\n",
    "        n = 10\n",
    "        seed = random.randint(0, n)\n",
    "\n",
    "        # 使用する7つの変数をランダムに取得する\n",
    "        # バギングをする際はそれぞれのサブサンプルで7つの変数を選択する．\n",
    "        random.seed(seed)\n",
    "        random_s = random.sample(list(X.columns), 7)\n",
    "        X = X[random_s]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "        # 必要な関数の定義\n",
    "\n",
    "        # 共分散行列の逆行列\n",
    "        def inv_cov(Z):\n",
    "            #標準化後のベクトルを入力する\n",
    "            #標準化した後なので相関行列と分散共分散行列は一致する\n",
    "            c = np.cov(Z.T)\n",
    "            return np.linalg.inv(c)\n",
    "\n",
    "        #マハラノビス汎距離\n",
    "        def cal_MD(Z, inv_C):\n",
    "            '''\n",
    "            Z:標準化したベクトル\n",
    "            inv_C:標準化後の共分散行列\n",
    "            '''\n",
    "            MD = []\n",
    "            for i in range(len(Z)):\n",
    "                _a = np.dot(Z[i], inv_C)\n",
    "                _MD = np.dot(_a, Z[i].T)\n",
    "                _MD = _MD / Z.shape[1]\n",
    "                MD.append(_MD)\n",
    "            return MD\n",
    "\n",
    "        # MTSを実行\n",
    "        def fit_MTS(X, y):\n",
    "            \n",
    "            # 正常データのみを使用して標準化\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X[y == 0])\n",
    "            normal_Z = scaler.transform(X[y == 0])\n",
    "            anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "            # 正常データのみを使用して共分散行列を計算\n",
    "            inv_C = inv_cov(normal_Z)\n",
    "\n",
    "            # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "                #１度目の仮のマハラノビス距離を計算\n",
    "                # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "                # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "                # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "            # 異常データと直交表を用いてSN比を計算\n",
    "            #L8直行表\n",
    "            df_l8 = pd.DataFrame([[1,1,1,1,1,1,1],[1,1,1,2,2,2,2],[1,2,2,1,1,2,2],[1,2,2,2,2,1,1],[2,1,2,1,2,1,2],[2,1,2,2,1,2,1],[2,2,1,1,2,2,1],[2,2,1,2,1,1,2]])\n",
    "            l8 = (df_l8==1).values\n",
    "\n",
    "            #異常データのマハラノビス距離\n",
    "            result = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "            for x, l8_row in enumerate(l8):\n",
    "                result[x] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row])\n",
    "\n",
    "            #SN比\n",
    "            sn = np.zeros(l8.shape[0])\n",
    "            for idx, row in enumerate(result):\n",
    "                sum_MD = 0\n",
    "                for i in range(len(row)):\n",
    "                    sum_MD += 1 / row[i]\n",
    "                sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "                \n",
    "            # SN比を利用し，不要と思われる変数を削除する\n",
    "            #変数選択\n",
    "            df_sn = pd.DataFrame(index=X.columns, columns=['SN比','残す'])\n",
    "            for i, clm in enumerate(X.columns):\n",
    "                df_sn.loc[df_sn.index == clm, 'SN比'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "                df_sn.loc[df_sn.index == clm, '残す'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]]) > 0\n",
    "            select_columns = df_sn[df_sn['残す']].index\n",
    "            \n",
    "            if len(select_columns) > 1:\n",
    "                # 選択変数でのスケーラーと共分散行列を計算\n",
    "                result_scaler = StandardScaler()\n",
    "                result_scaler.fit(X[select_columns][y == 0])\n",
    "                result_Z = result_scaler.transform(X[select_columns][y == 0])\n",
    "                result_inv_C = inv_cov(result_Z)\n",
    "            else:\n",
    "                select_columns = df_sn['SN比'].astype(float).idxmax()\n",
    "                result_scaler = 0\n",
    "                result_inv_C = 0\n",
    "\n",
    "            # 単位空間のスケーラーと共分散行列と選択した変数を出力\n",
    "            return result_scaler, result_inv_C, select_columns\n",
    "\n",
    "        # 新しいデータのマハラノビス距離を計算する\n",
    "        def predict_MTS(X, scaler, inv_C, select_columns):\n",
    "            Z = scaler.transform(X[select_columns])\n",
    "            MD = cal_MD(Z, inv_C)\n",
    "            return MD\n",
    "\n",
    "        # 閾値をジニ係数が最小になるように決定する\n",
    "        def determine_threshold(y_true, y_pred):\n",
    "            df_pred = pd.DataFrame(y_true)\n",
    "            df_pred['pred'] = y_pred\n",
    "            df_pred = df_pred.sort_values('pred').reset_index(drop=True)\n",
    "\n",
    "            min_gini = np.inf\n",
    "            threshold = 0\n",
    "            for i in range(len(df_pred)):\n",
    "                \n",
    "                neg = df_pred.iloc[:i+1]\n",
    "                pos = df_pred.iloc[i:]\n",
    "\n",
    "                p_neg = sum(neg[y_true.name]) / len(neg)\n",
    "                gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "                p_pos = sum(pos[y_true.name]) / len(pos)\n",
    "                gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "                gini_split = (len(neg) / len(df_pred) * gini_neg) + (len(pos) / len(df_pred) * gini_pos)\n",
    "\n",
    "                if min_gini > gini_split:\n",
    "                    min_gini = gini_split\n",
    "                    threshold = df_pred.iloc[i]['pred']\n",
    "                    threshold_idx = i\n",
    "\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            # print('Best paramater')\n",
    "            # print(threshold_idx, min_gini, threshold)\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            \n",
    "            return threshold\n",
    "\n",
    "\n",
    "        result_scaler, result_inv_C, select_columns = fit_MTS(X_train, y_train)\n",
    "\n",
    "        if result_scaler != 0:\n",
    "            y_train_pred = predict_MTS(X_train, result_scaler, result_inv_C, select_columns)\n",
    "        else:\n",
    "            y_train_pred = X_train[select_columns]\n",
    "            \n",
    "        threshold = determine_threshold(y_train, y_train_pred)\n",
    "    \n",
    "        if result_scaler != 0:\n",
    "            Z = result_scaler.transform(X_test[select_columns])\n",
    "            MD = cal_MD(Z, result_inv_C)\n",
    "            y_pred = MD > threshold\n",
    "            y_proba = MD\n",
    "        else:\n",
    "            y_pred = X_test[select_columns] > threshold\n",
    "            y_proba = X_test[select_columns]\n",
    "\n",
    "        MTS_result_df['AUC'][m] = roc_auc_score(y_test, y_proba)\n",
    "        MTS_result_df['accuracy'][m] = accuracy_score(y_test, y_pred)\n",
    "        MTS_result_df['recall'][m] = recall_score(y_test, y_pred)\n",
    "        MTS_result_df['precision'][m] = precision_score(y_test, y_pred)\n",
    "        MTS_result_df['gmeans'][m] = np.sqrt(recall_score(y_test, y_pred) * precision_score(y_test, y_pred))\n",
    "        MTS_result_df['RS'][m] = recall_score(y_test, y_pred) / precision_score(y_test, y_pred)\n",
    "    MTS_result_df.to_csv(f'../data/MTS_{data}_result.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 26.75it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.37it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 28.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 28.22it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 28.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 27.78it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.36it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.36it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.38it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.38it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 10/10 [01:12<00:00,  7.28s/it]\n",
      "100%|██████████| 10/10 [01:30<00:00,  9.06s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.19s/it]\n",
      "100%|██████████| 10/10 [01:32<00:00,  9.28s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.19s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  9.00s/it]\n",
      "100%|██████████| 10/10 [01:30<00:00,  9.03s/it]\n",
      "100%|██████████| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.93s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.94s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.95s/it]\n",
      "100%|██████████| 10/10 [15:06<00:00, 90.62s/it]\n"
     ]
    }
   ],
   "source": [
    "n_experiment = 10\n",
    "data_list = ['wine', 'abalone','letter']\n",
    "\n",
    "for data in data_list:\n",
    "    MTSBag_result_df = pd.DataFrame(\n",
    "                        columns=['AUC', 'accuracy', 'recall', 'precision', 'gmeans', 'RS'],\n",
    "                        index=range(n_experiment))\n",
    "    for m in tqdm(range(n_experiment)):\n",
    "        \n",
    "        select_data = data\n",
    "\n",
    "            \n",
    "\n",
    "        # パラメータ\n",
    "        n_estimators = 10\n",
    "        max_samples = 0.5\n",
    "\n",
    "\n",
    "        # データ取得\n",
    "        if select_data == 'letter':\n",
    "            # データの取得\n",
    "            df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "            # Aのみを判定するため，Aを0，A以外を1にした．\n",
    "            # 少数派のAを正常，その他を異常データと定義\n",
    "            df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "            #Xとyを入力\n",
    "            X = df[range(1,17)]\n",
    "            y = df[0]\n",
    "\n",
    "        elif select_data == 'wine':\n",
    "            \n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('wine.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            column_names = ['Alcohol',\n",
    "            'Malic acid',\n",
    "            'Ash',\n",
    "            'Alcalinity of ash',\n",
    "            'Magnesium',\n",
    "            'Total phenols',\n",
    "            'Flavanoids',\n",
    "            'Nonflavanoid phenols',\n",
    "            'Proanthocyanins',\n",
    "            'Color intensity',\n",
    "            'Hue',\n",
    "            'OD280/OD315 of diluted wines',\n",
    "            'Proline' \n",
    "            ]\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=column_names)\n",
    "            raw_data['y'] = raw_data.index\n",
    "            raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "            raw_data['y'] = raw_data['y'].apply(lambda x: 0 if x == 3 else 1)\n",
    "\n",
    "            X = raw_data.drop('y', axis=1)\n",
    "            y = raw_data['y']\n",
    "\n",
    "        elif select_data == 'abalone':\n",
    "\n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('abalone.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=range(8)).reset_index(drop=True)\n",
    "\n",
    "            raw_data[7] = raw_data[7].apply(lambda x: 1 if x > 4 else 0)\n",
    "\n",
    "\n",
    "            X = raw_data.drop(7, axis=1)\n",
    "            y = raw_data[7]\n",
    "\n",
    "        else:\n",
    "            print('そのデータはありません')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "        # 必要な関数の定義\n",
    "\n",
    "        # 共分散行列の逆行列\n",
    "        def inv_cov(Z):\n",
    "            #標準化後のベクトルを入力する\n",
    "            #標準化した後なので相関行列と分散共分散行列は一致する\n",
    "            c = np.cov(Z.T)\n",
    "            return np.linalg.inv(c)\n",
    "\n",
    "        #マハラノビス汎距離\n",
    "        def cal_MD(Z, inv_C):\n",
    "            '''\n",
    "            Z:標準化したベクトル\n",
    "            inv_C:標準化後の共分散行列\n",
    "            '''\n",
    "            MD = np.zeros(len(Z))\n",
    "            for i in range(len(Z)):\n",
    "                _a = np.dot(Z[i], inv_C)\n",
    "                _MD = np.dot(_a, Z[i].T)\n",
    "                _MD = _MD / Z.shape[1]\n",
    "                MD[i] = _MD\n",
    "            return MD\n",
    "\n",
    "        # MTSを実行\n",
    "        def fit_MTS(X, y):\n",
    "            \n",
    "            # 正常データのみを使用して標準化\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X[y == 0])\n",
    "            normal_Z = scaler.transform(X[y == 0])\n",
    "            anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "            # 正常データのみを使用して共分散行列を計算\n",
    "            inv_C = inv_cov(normal_Z)\n",
    "\n",
    "            # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "                #１度目の仮のマハラノビス距離を計算\n",
    "                # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "                # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "                # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "            # 異常データと直交表を用いてSN比を計算\n",
    "            #L8直行表\n",
    "            l8 = np.array([\n",
    "                [1,1,1,1,1,1,1],\n",
    "                [1,1,1,2,2,2,2],\n",
    "                [1,2,2,1,1,2,2],\n",
    "                [1,2,2,2,2,1,1],\n",
    "                [2,1,2,1,2,1,2],\n",
    "                [2,1,2,2,1,2,1],\n",
    "                [2,2,1,1,2,2,1],\n",
    "                [2,2,1,2,1,1,2]\n",
    "                ])\n",
    "            l8 = (l8 == 1)\n",
    "\n",
    "            #異常データのマハラノビス距離\n",
    "            result = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "            for i, l8_row in enumerate(l8):\n",
    "                result[i] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row])\n",
    "\n",
    "            #SN比\n",
    "            sn = np.zeros(l8.shape[0])\n",
    "            for idx, row in enumerate(result):\n",
    "                sum_MD = 0\n",
    "                for i in range(len(row)):\n",
    "                    sum_MD += 1 / row[i]\n",
    "                sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "                \n",
    "            # SN比を利用し，不要と思われる変数を削除する\n",
    "            #変数選択\n",
    "            df_sn = pd.DataFrame(index=X.columns, columns=['SN比','残す'])\n",
    "            for i, clm in enumerate(X.columns):\n",
    "                df_sn.loc[df_sn.index == clm, 'SN比'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "                df_sn.loc[df_sn.index == clm, '残す'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]]) > 0\n",
    "            #使用した変数を保存\n",
    "            select_columns = df_sn[df_sn['残す']].index\n",
    "            \n",
    "            if len(select_columns) > 1:\n",
    "                # 選択変数でのスケーラーと共分散行列を計算\n",
    "                result_scaler = StandardScaler()\n",
    "                result_scaler.fit(X[select_columns][y == 0])\n",
    "                result_Z = result_scaler.transform(X[select_columns][y == 0])\n",
    "                result_inv_C = inv_cov(result_Z)\n",
    "            else:\n",
    "                select_columns = df_sn['SN比'].astype(float).idxmax()\n",
    "                result_scaler = 0\n",
    "                result_inv_C = 0\n",
    "\n",
    "            # 単位空間のスケーラーと共分散行列と選択した変数を出力\n",
    "            return result_scaler, result_inv_C, select_columns\n",
    "\n",
    "        # 新しいデータのマハラノビス距離を計算する\n",
    "        def predict_MTS(X, scaler, inv_C, select_columns):\n",
    "            Z = scaler.transform(X[select_columns])\n",
    "            MD = cal_MD(Z, inv_C)\n",
    "            return MD\n",
    "\n",
    "        # 閾値をジニ係数が最小になるように決定する\n",
    "        def determine_threshold(y_true, y_pred):\n",
    "            df_pred = pd.DataFrame(y_true)\n",
    "            df_pred['pred'] = y_pred\n",
    "            df_pred = df_pred.sort_values('pred').reset_index(drop=True)\n",
    "\n",
    "            min_gini = np.inf\n",
    "            threshold = 0\n",
    "            for i in range(len(df_pred)):\n",
    "                \n",
    "                neg = df_pred.iloc[:i+1]\n",
    "                pos = df_pred.iloc[i:]\n",
    "\n",
    "                p_neg = sum(neg[y_true.name]) / len(neg)\n",
    "                gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "                p_pos = sum(pos[y_true.name]) / len(pos)\n",
    "                gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "                gini_split = (len(neg) / len(df_pred) * gini_neg) + (len(pos) / len(df_pred) * gini_pos)\n",
    "\n",
    "                if min_gini > gini_split:\n",
    "                    min_gini = gini_split\n",
    "                    threshold = df_pred.iloc[i]['pred']\n",
    "                    threshold_idx = i\n",
    "\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            # print('Best paramater')\n",
    "            # print(threshold_idx, min_gini, threshold)\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "            # print('AUC : ', roc_auc_score(y_true.values, y_pred))\n",
    "\n",
    "            # recall = df_pred.iloc[threshold_idx + 1:][y_true.name].sum() / df_pred[y_true.name].sum()\n",
    "            # print('recall : ', recall)\n",
    "\n",
    "            # precision = df_pred.iloc[threshold_idx + 1:][y_true.name].mean()\n",
    "            # print('precision :', precision)\n",
    "\n",
    "            # g_mean = np.sqrt(recall * precision)\n",
    "            # print('g_mean : ', g_mean)\n",
    "\n",
    "            # RS = recall / precision\n",
    "            # print('RS : ', RS)\n",
    "            return threshold\n",
    "\n",
    "        def predict_MTSBag(X, scaler, inv_C, select_columns, threshold):\n",
    "            result = np.ndarray((K, len(X_test)), dtype=bool)\n",
    "            for i in range(K):\n",
    "                if scaler[i] != 0:\n",
    "                    Z = scaler[i].transform(X[select_columns[i]])\n",
    "                    MD = cal_MD(Z, inv_C[i])\n",
    "                    result[i] = MD > threshold[i]\n",
    "                else:\n",
    "                    result[i] = X[select_columns[i]] > threshold[i]\n",
    "            return result.sum(axis=0) / K, result.sum(axis=0) > (K/2)\n",
    "\n",
    "\n",
    "\n",
    "        # 実行するところ\n",
    "\n",
    "        # K:再標本化の回数 SIZE:再標本化されたもののサンプルサイズ\n",
    "        K = n_estimators\n",
    "        SIZE = int(len(X) * max_samples)\n",
    "\n",
    "        # 予測に必要なパラメータ\n",
    "        select_columns = [0] * K\n",
    "        result_scaler = [0] * K\n",
    "        result_inv_C = [0] * K\n",
    "        threshold = [0] * K\n",
    "\n",
    "        for i in tqdm(range(K)):\n",
    "            # bootstrap sampling\n",
    "            resampled_data_x, resampled_data_y = resample(X_train, y_train, n_samples = SIZE)\n",
    "            random_s = random.sample(list(resampled_data_x.columns), 7)\n",
    "            resampled_data_x = resampled_data_x[random_s]\n",
    "\n",
    "            result_scaler[i], result_inv_C[i], select_columns[i] = fit_MTS(resampled_data_x, resampled_data_y)\n",
    "\n",
    "            if result_scaler[i] != 0:\n",
    "                y_pred = predict_MTS(resampled_data_x, result_scaler[i], result_inv_C[i], select_columns[i])\n",
    "            else:\n",
    "                y_pred = resampled_data_x[select_columns[i]]\n",
    "\n",
    "            threshold[i] = determine_threshold(resampled_data_y, y_pred)\n",
    "            \n",
    "\n",
    "        y_proba, y_pred = predict_MTSBag(X_test, result_scaler, result_inv_C, select_columns, threshold)\n",
    "\n",
    "        MTSBag_result_df['AUC'][m] = roc_auc_score(y_test, y_proba)\n",
    "        MTSBag_result_df['accuracy'][m] = accuracy_score(y_test, y_pred)\n",
    "        MTSBag_result_df['recall'][m] = recall_score(y_test, y_pred)\n",
    "        MTSBag_result_df['precision'][m] = precision_score(y_test, y_pred)\n",
    "        MTSBag_result_df['gmeans'][m] = np.sqrt(recall_score(y_test, y_pred) * precision_score(y_test, y_pred))\n",
    "        MTSBag_result_df['RS'][m] = recall_score(y_test, y_pred) / precision_score(y_test, y_pred)\n",
    "    MTSBag_result_df.to_csv(f'../data/MTSBag_{data}_result.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTEMTSBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.50it/s]\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.56s/it]\n",
      "100%|██████████| 10/10 [15:46<00:00, 94.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "n_experiment = 10\n",
    "data_list = ['wine', 'abalone','letter']\n",
    "\n",
    "for data in data_list:\n",
    "    SMOTEMTSBag_result_df = pd.DataFrame(\n",
    "                        columns=['AUC', 'accuracy', 'recall', 'precision', 'gmeans', 'RS'],\n",
    "                        index=range(n_experiment))\n",
    "    for m in tqdm(range(n_experiment)):\n",
    "        \n",
    "        select_data = data\n",
    "\n",
    "            \n",
    "\n",
    "        # パラメータ\n",
    "        n_estimators = 10\n",
    "        max_samples = 0.5\n",
    "\n",
    "\n",
    "        # データ取得\n",
    "        if select_data == 'letter':\n",
    "            # データの取得\n",
    "            df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "            # Aのみを判定するため，Aを0，A以外を1にした．\n",
    "            # 少数派のAを正常，その他を異常データと定義\n",
    "            df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "            #Xとyを入力\n",
    "            X = df[range(1,17)]\n",
    "            y = df[0]\n",
    "\n",
    "        elif select_data == 'wine':\n",
    "            \n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('wine.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            column_names = ['Alcohol',\n",
    "            'Malic acid',\n",
    "            'Ash',\n",
    "            'Alcalinity of ash',\n",
    "            'Magnesium',\n",
    "            'Total phenols',\n",
    "            'Flavanoids',\n",
    "            'Nonflavanoid phenols',\n",
    "            'Proanthocyanins',\n",
    "            'Color intensity',\n",
    "            'Hue',\n",
    "            'OD280/OD315 of diluted wines',\n",
    "            'Proline' \n",
    "            ]\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=column_names)\n",
    "            raw_data['y'] = raw_data.index\n",
    "            raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "            raw_data['y'] = raw_data['y'].apply(lambda x: 0 if x == 3 else 1)\n",
    "\n",
    "            X = raw_data.drop('y', axis=1)\n",
    "            y = raw_data['y']\n",
    "\n",
    "        elif select_data == 'abalone':\n",
    "\n",
    "            dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "\n",
    "            # ファイルのダウンロード\n",
    "            dataset_path = tf.keras.utils.get_file('abalone.data', dataset_url)\n",
    "\n",
    "            # print(dataset_path)\n",
    "\n",
    "            raw_data = pd.read_csv(dataset_path, names=range(8)).reset_index(drop=True)\n",
    "\n",
    "            raw_data[7] = raw_data[7].apply(lambda x: 1 if x > 4 else 0)\n",
    "\n",
    "\n",
    "            X = raw_data.drop(7, axis=1)\n",
    "            y = raw_data[7]\n",
    "\n",
    "        else:\n",
    "            print('そのデータはありません')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "        # 必要な関数の定義\n",
    "\n",
    "        # 共分散行列の逆行列\n",
    "        def inv_cov(Z):\n",
    "            #標準化後のベクトルを入力する\n",
    "            #標準化した後なので相関行列と分散共分散行列は一致する\n",
    "            c = np.cov(Z.T)\n",
    "            return np.linalg.inv(c)\n",
    "\n",
    "        #マハラノビス汎距離\n",
    "        def cal_MD(Z, inv_C):\n",
    "            '''\n",
    "            Z:標準化したベクトル\n",
    "            inv_C:標準化後の共分散行列\n",
    "            '''\n",
    "            MD = np.zeros(len(Z))\n",
    "            for i in range(len(Z)):\n",
    "                _a = np.dot(Z[i], inv_C)\n",
    "                _MD = np.dot(_a, Z[i].T)\n",
    "                _MD = _MD / Z.shape[1]\n",
    "                MD[i] = _MD\n",
    "            return MD\n",
    "\n",
    "        # MTSを実行\n",
    "        def fit_MTS(X, y):\n",
    "            \n",
    "            # 正常データのみを使用して標準化\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X[y == 0])\n",
    "            normal_Z = scaler.transform(X[y == 0])\n",
    "            anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "            # 正常データのみを使用して共分散行列を計算\n",
    "            inv_C = inv_cov(normal_Z)\n",
    "\n",
    "            # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "                #１度目の仮のマハラノビス距離を計算\n",
    "                # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "                # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "                # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "            # 異常データと直交表を用いてSN比を計算\n",
    "            #L8直行表\n",
    "            l8 = np.array([\n",
    "                [1,1,1,1,1,1,1],\n",
    "                [1,1,1,2,2,2,2],\n",
    "                [1,2,2,1,1,2,2],\n",
    "                [1,2,2,2,2,1,1],\n",
    "                [2,1,2,1,2,1,2],\n",
    "                [2,1,2,2,1,2,1],\n",
    "                [2,2,1,1,2,2,1],\n",
    "                [2,2,1,2,1,1,2]\n",
    "                ])\n",
    "            l8 = (l8 == 1)\n",
    "\n",
    "            #異常データのマハラノビス距離\n",
    "            result = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "            for i, l8_row in enumerate(l8):\n",
    "                result[i] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row])\n",
    "\n",
    "            #SN比\n",
    "            sn = np.zeros(l8.shape[0])\n",
    "            for idx, row in enumerate(result):\n",
    "                sum_MD = 0\n",
    "                for i in range(len(row)):\n",
    "                    sum_MD += 1 / row[i]\n",
    "                sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "                \n",
    "            # SN比を利用し，不要と思われる変数を削除する\n",
    "            #変数選択\n",
    "            df_sn = pd.DataFrame(index=X.columns, columns=['SN比','残す'])\n",
    "            for i, clm in enumerate(X.columns):\n",
    "                df_sn.loc[df_sn.index == clm, 'SN比'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "                df_sn.loc[df_sn.index == clm, '残す'] = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]]) > 0\n",
    "            #使用した変数を保存\n",
    "            select_columns = df_sn[df_sn['残す']].index\n",
    "            \n",
    "            if len(select_columns) > 1:\n",
    "                # 選択変数でのスケーラーと共分散行列を計算\n",
    "                result_scaler = StandardScaler()\n",
    "                result_scaler.fit(X[select_columns][y == 0])\n",
    "                result_Z = result_scaler.transform(X[select_columns][y == 0])\n",
    "                result_inv_C = inv_cov(result_Z)\n",
    "            else:\n",
    "                select_columns = df_sn['SN比'].astype(float).idxmax()\n",
    "                result_scaler = 0\n",
    "                result_inv_C = 0\n",
    "\n",
    "            # 単位空間のスケーラーと共分散行列と選択した変数を出力\n",
    "            return result_scaler, result_inv_C, select_columns\n",
    "\n",
    "        # 新しいデータのマハラノビス距離を計算する\n",
    "        def predict_MTS(X, scaler, inv_C, select_columns):\n",
    "            Z = scaler.transform(X[select_columns])\n",
    "            MD = cal_MD(Z, inv_C)\n",
    "            return MD\n",
    "\n",
    "        # 閾値をジニ係数が最小になるように決定する\n",
    "        def determine_threshold(y_true, y_pred):\n",
    "            df_pred = pd.DataFrame(y_true)\n",
    "            df_pred['pred'] = y_pred\n",
    "            df_pred = df_pred.sort_values('pred').reset_index(drop=True)\n",
    "\n",
    "            min_gini = np.inf\n",
    "            threshold = 0\n",
    "            for i in range(len(df_pred)):\n",
    "                \n",
    "                neg = df_pred.iloc[:i+1]\n",
    "                pos = df_pred.iloc[i:]\n",
    "\n",
    "                p_neg = sum(neg[y_true.name]) / len(neg)\n",
    "                gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "                p_pos = sum(pos[y_true.name]) / len(pos)\n",
    "                gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "                gini_split = (len(neg) / len(df_pred) * gini_neg) + (len(pos) / len(df_pred) * gini_pos)\n",
    "\n",
    "                if min_gini > gini_split:\n",
    "                    min_gini = gini_split\n",
    "                    threshold = df_pred.iloc[i]['pred']\n",
    "                    threshold_idx = i\n",
    "\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            # print('Best paramater')\n",
    "            # print(threshold_idx, min_gini, threshold)\n",
    "            # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "            # print('AUC : ', roc_auc_score(y_true.values, y_pred))\n",
    "\n",
    "            # recall = df_pred.iloc[threshold_idx + 1:][y_true.name].sum() / df_pred[y_true.name].sum()\n",
    "            # print('recall : ', recall)\n",
    "\n",
    "            # precision = df_pred.iloc[threshold_idx + 1:][y_true.name].mean()\n",
    "            # print('precision :', precision)\n",
    "\n",
    "            # g_mean = np.sqrt(recall * precision)\n",
    "            # print('g_mean : ', g_mean)\n",
    "\n",
    "            # RS = recall / precision\n",
    "            # print('RS : ', RS)\n",
    "            return threshold\n",
    "\n",
    "        def predict_MTSBag(X, scaler, inv_C, select_columns, threshold):\n",
    "            result = np.ndarray((K, len(X_test)), dtype=bool)\n",
    "            for i in range(K):\n",
    "                if scaler[i] != 0:\n",
    "                    Z = scaler[i].transform(X[select_columns[i]])\n",
    "                    MD = cal_MD(Z, inv_C[i])\n",
    "                    result[i] = MD > threshold[i]\n",
    "                else:\n",
    "                    result[i] = X[select_columns[i]] > threshold[i]\n",
    "            return result.sum(axis=0) / K, result.sum(axis=0) > (K/2)\n",
    "\n",
    "\n",
    "\n",
    "        # 実行するところ\n",
    "\n",
    "        # K:再標本化の回数 SIZE:再標本化されたもののサンプルサイズ\n",
    "        K = n_estimators\n",
    "        SIZE = int(len(X) * max_samples)\n",
    "\n",
    "        # 予測に必要なパラメータ\n",
    "        select_columns = [0] * K\n",
    "        result_scaler = [0] * K\n",
    "        result_inv_C = [0] * K\n",
    "        threshold = [0] * K\n",
    "\n",
    "        # SMOTEを実行\n",
    "        sampler = SMOTE()\n",
    "        SMOTE_X, SMOTE_y = sampler.fit_resample(X=X_train, y=y_train)\n",
    "        for i in range(K):\n",
    "            # bootstrap sampling\n",
    "            resampled_data_x, resampled_data_y = resample(SMOTE_X, SMOTE_y, n_samples = SIZE)\n",
    "            random_s = random.sample(list(resampled_data_x.columns), 7)\n",
    "            resampled_data_x = resampled_data_x[random_s]\n",
    "\n",
    "            result_scaler[i], result_inv_C[i], select_columns[i] = fit_MTS(resampled_data_x, resampled_data_y)\n",
    "\n",
    "            if result_scaler[i] != 0:\n",
    "                y_pred = predict_MTS(resampled_data_x, result_scaler[i], result_inv_C[i], select_columns[i])\n",
    "            else:\n",
    "                y_pred = resampled_data_x[select_columns[i]]\n",
    "\n",
    "            threshold[i] = determine_threshold(resampled_data_y, y_pred)\n",
    "            \n",
    "\n",
    "        y_proba, y_pred = predict_MTSBag(X_test, result_scaler, result_inv_C, select_columns, threshold)\n",
    "\n",
    "        SMOTEMTSBag_result_df['AUC'][m] = roc_auc_score(y_test, y_proba)\n",
    "        SMOTEMTSBag_result_df['accuracy'][m] = accuracy_score(y_test, y_pred)\n",
    "        SMOTEMTSBag_result_df['recall'][m] = recall_score(y_test, y_pred)\n",
    "        SMOTEMTSBag_result_df['precision'][m] = precision_score(y_test, y_pred)\n",
    "        SMOTEMTSBag_result_df['gmeans'][m] = np.sqrt(recall_score(y_test, y_pred) * precision_score(y_test, y_pred))\n",
    "        SMOTEMTSBag_result_df['RS'][m] = recall_score(y_test, y_pred) / precision_score(y_test, y_pred)\n",
    "    SMOTEMTSBag_result_df.to_csv(f'../data/SMOTEMTSBag_{data}_result.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7e6621f5c0e725993c5f5dd1734f3da8dc8c958ed2c46496e37b878d46070df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('convenient': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
