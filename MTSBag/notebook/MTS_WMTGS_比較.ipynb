{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSとWMTGSの比較\n",
    "- MTSとWMTGSのきれいな関数も作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.dates as mdates\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "#Aのみを判定するため，Aを１，A以外を0にした．\n",
    "df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "#Xとyを入力\n",
    "X = df[range(1,17)]\n",
    "y = df[0]\n",
    "\n",
    "#バギング側の話\n",
    "#ブートストラップサンプリングの個数\n",
    "n = 10\n",
    "seed = random.randint(0, n)\n",
    "\n",
    "#使用する7つの変数をランダムに取得する\n",
    "random.seed(2)\n",
    "random_s = random.sample(list(X.columns), 7)\n",
    "use_X = X[random_s]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(use_X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な関数の定義\n",
    "\n",
    "# 共分散行列の逆行列\n",
    "def inv_cov(Z):\n",
    "    #標準化後のベクトルを入力する\n",
    "    #標準化した後なので相関行列と分散共分散行列は一致する\n",
    "    c = np.cov(Z.T)\n",
    "    return np.linalg.pinv(c)\n",
    "\n",
    "#マハラノビス汎距離\n",
    "def cal_MD(Z, inv_C):\n",
    "    '''\n",
    "    Z:標準化したベクトル\n",
    "    inv_C:分散共分散行列の逆行列\n",
    "    '''\n",
    "    MD = np.zeros(len(Z))\n",
    "    for i in range(len(Z)):\n",
    "        _a = np.dot(Z[i], inv_C)\n",
    "        _MD = np.dot(_a, Z[i].T)\n",
    "        _MD = _MD / Z.shape[1]\n",
    "        MD[i] = _MD\n",
    "    return MD\n",
    "    \n",
    "# 閾値をジニ係数が最小になるように決定する\n",
    "def determine_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        y_true: trainデータのラベルを入力\n",
    "        y_proba: trainデータの異常度（縮小モデルのMD）を入力\n",
    "    output: threshold\n",
    "    \"\"\"\n",
    "    df_ = pd.DataFrame(y_true)\n",
    "    df_['proba'] = y_proba\n",
    "    df_ = df_.sort_values('proba').reset_index(drop=True)\n",
    "\n",
    "    min_gini = np.inf\n",
    "    threshold = 0\n",
    "    for i in range(len(df_)):\n",
    "        \n",
    "        neg = df_.iloc[:i+1]\n",
    "        pos = df_.iloc[i:]\n",
    "\n",
    "        p_neg = sum(neg[y_true.name]) / len(neg)\n",
    "        gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "        p_pos = sum(pos[y_true.name]) / len(pos)\n",
    "        gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "        gini_split = (len(neg) / len(df_) * gini_neg) + (len(pos) / len(df_) * gini_pos)\n",
    "\n",
    "        if min_gini > gini_split:\n",
    "            min_gini = gini_split\n",
    "            threshold = df_.iloc[i]['proba']\n",
    "            threshold_idx = i\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTS\n",
    "1. 単位空間の作成\n",
    "    1. Xを正常データのみで標準化してZを取得\n",
    "    2. 正常データのみで共分散行列の逆行列Inv_Cを取得\n",
    "2. 縮小単位空間作成のための変数選択\n",
    "    1. 異常データ（anomaly_Z）を用いてSN比を算出\n",
    "    2. 直交表(現状はL8のみ)を用いて各変数の効果ゲインを算出\n",
    "    3. 効果ゲインが負の変数を削除し，縮小単位空間を作成\n",
    "3. 縮小単位空間の閾値決定\n",
    "    1. gini係数が最小となる閾値を算出\n",
    "4. 新しいデータの予測\n",
    "    1. 縮小単位空間によって新しいデータのマハラノビス距離を算出し，異常度とする．\n",
    "    2. その異常度が閾値を超えたら異常と予測する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTSを実行\n",
    "def fit_MTS(X, y):\n",
    "    \"\"\"\n",
    "    input: X, y\n",
    "    output: reduced_model_scaler, reduced_model_inv_C, select_columns\n",
    "\n",
    "    reduced_model_scaler: 縮小モデルのスケーラー\n",
    "    reduced_model_inv_C: 縮小モデルの共分散行列の逆行列\n",
    "    select_columns: 選択された変数\n",
    "    \"\"\"\n",
    "    # 正常データのみを使用して標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[y == 0])\n",
    "    normal_Z = scaler.transform(X[y == 0])\n",
    "    anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "    # 正常データのみを使用して共分散行列を計算\n",
    "    inv_C = inv_cov(normal_Z)\n",
    "\n",
    "    # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "        #１度目の仮のマハラノビス距離を計算\n",
    "        # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "        # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "        # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "    # 異常データと直交表を用いてSN比を計算\n",
    "    #L8直行表\n",
    "    l8 = np.array([\n",
    "        [1,1,1,1,1,1,1],\n",
    "        [1,1,1,2,2,2,2],\n",
    "        [1,2,2,1,1,2,2],\n",
    "        [1,2,2,2,2,1,1],\n",
    "        [2,1,2,1,2,1,2],\n",
    "        [2,1,2,2,1,2,1],\n",
    "        [2,2,1,1,2,2,1],\n",
    "        [2,2,1,2,1,1,2]\n",
    "        ])\n",
    "    l8 = (l8 == 1)\n",
    "\n",
    "    # 異常データのマハラノビス距離\n",
    "    anomaly_MD = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "    for i, l8_row in enumerate(l8):\n",
    "        anomaly_MD[i] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row]) # 正常データのinv_Cを使う必要がある\n",
    "    print(anomaly_MD)\n",
    "\n",
    "    # SN比の算出\n",
    "    sn = np.zeros(l8.shape[0])\n",
    "    for idx, row in enumerate(anomaly_MD):\n",
    "        sum_MD = 0\n",
    "        for row_i in row:\n",
    "            sum_MD += 1 / row_i\n",
    "        sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "        \n",
    "    # SN比を利用し，不要と思われる変数を削除する\n",
    "    # 変数選択\n",
    "    df_gain = pd.DataFrame(index=X.columns, columns=['効果ゲイン','残す'])\n",
    "    for i, clm in enumerate(X.columns):\n",
    "        gain = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "        df_gain.loc[df_gain.index == clm, '効果ゲイン'] = gain\n",
    "        df_gain.loc[df_gain.index == clm, '残す'] = gain > 0\n",
    "    # 選択された変数を保存\n",
    "    select_columns = df_gain[df_gain['残す']].index\n",
    "    \n",
    "    # 選択された変数が1つ以下の場合の例外処理\n",
    "    if len(select_columns) > 1:\n",
    "        # 縮小モデルでのスケーラーと共分散行列を計算\n",
    "        reduced_model_scaler = StandardScaler()\n",
    "        reduced_model_scaler.fit(X[select_columns][y == 0])\n",
    "        reduced_model_normal_Z = reduced_model_scaler.transform(X[select_columns][y == 0])\n",
    "        reduced_model_inv_C = inv_cov(reduced_model_normal_Z)\n",
    "    # 選択された変数が一つ以下の場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        select_columns = df_gain['効果ゲイン'].astype(float).idxmax()\n",
    "        reduced_model_scaler = X[select_columns][y == 0].mean()\n",
    "        reduced_model_inv_C = X[select_columns][y == 0].std()\n",
    "\n",
    "    # 縮小モデルのスケーラーと共分散行列と選択した変数を出力\n",
    "    return reduced_model_scaler, reduced_model_inv_C, select_columns\n",
    "# 縮小モデルによってマハラノビス距離を計算する\n",
    "def cal_MD_by_reduced_model(X, reduced_model_scaler, reduced_model_inv_C, select_columns):\n",
    "    # select_columnsがfloatになることがある？\n",
    "    if type(reduced_model_scaler) == StandardScaler:\n",
    "        Z = reduced_model_scaler.transform(X[select_columns])\n",
    "        MD = cal_MD(Z, reduced_model_inv_C)\n",
    "    # 変数が一つしか選択されなかった場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        MD = ((X[select_columns] - reduced_model_scaler) / reduced_model_inv_C) ** 2\n",
    "    return MD\n",
    "def predict_MTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, threshold):\n",
    "    proba = cal_MD_by_reduced_model(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns)\n",
    "    pred = proba > threshold\n",
    "    return proba, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.85357165  4.70440543 13.63909846 ...  2.43648471  0.49234346\n",
      "   2.35276805]\n",
      " [ 6.1895112   0.96161499  5.55157793 ...  3.3485277   1.65387734\n",
      "   2.14852772]\n",
      " [ 3.80726466  3.34197953  2.37698576 ...  3.9173113   0.21633928\n",
      "   2.43517513]\n",
      " ...\n",
      " [ 2.3568608   2.94109136 23.65734927 ...  2.18872417  1.02120659\n",
      "   0.82223985]\n",
      " [ 9.44715883 10.83044959 26.97138004 ...  3.39171349  1.70713183\n",
      "   3.74255775]\n",
      " [ 6.28922078  1.56891532  9.15262627 ...  0.94783549  1.21761351\n",
      "   1.28464218]]\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, reduced_model_inv_C, select_columns = fit_MTS(X_train, y_train)\n",
    "y_proba_train = cal_MD_by_reduced_model(X_train, reduced_model_scaler, reduced_model_inv_C, select_columns)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_MTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, threshold)\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8830043006569847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([10, 14, 16, 12], dtype='int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果がおかしい？？？→データがおかしかった！！！解決\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMTGS\n",
    "1. 単位空間の作成\n",
    "    1. Xを正常データのみで標準化してZを取得\n",
    "    2. 正常データのみで共分散行列の逆行列Inv_Cを取得\n",
    "2. 縮小単位空間作成のための変数重みづけ\n",
    "    1. 異常データ（anomaly_Z）の転置行列からグラムシュミット直交化法によって直交ベクトル（gram_vec）を取得\n",
    "    2. 直交ベクトル（gram_vec）を用いてSN比を算出\n",
    "    3. 直交表(現状はL8のみ)を用いて各変数の効果ゲインを算出\n",
    "    4. 効果ゲインが負の変数を削除する．\n",
    "    5. 効果ゲインが正の変数グループにおいて効果ゲインによる重みを算出する．\n",
    "3. 縮小単位空間の閾値決定\n",
    "    1. その重みを付与した異常度を算出\n",
    "    2. gini係数が最小となる閾値を算出\n",
    "4. 新しいデータの予測\n",
    "    1. 重みづけした縮小単位空間によって新しいデータのマハラノビス距離を算出し，異常度とする．\n",
    "    2. その異常度が閾値を超えたら異常と予測する．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#グラムシュミット法によるマハラノビス汎距離\n",
    "def gram_schmidt_cal_MD(Z, feature_weight=[1.0]*100):\n",
    "    '''\n",
    "    Z:標準化したベクトル\n",
    "    '''\n",
    "    gram_vec, _ = np.linalg.qr(Z)\n",
    "    ips = np.diag(np.cov(gram_vec.T))\n",
    "    k = gram_vec.shape[1]\n",
    "    MD = np.zeros(len(Z))\n",
    "    \n",
    "    for i, one_gram_vec in enumerate(gram_vec):\n",
    "        _MD = 0\n",
    "        for q, u in enumerate(one_gram_vec):\n",
    "            _MD += feature_weight[q] * (u**2 / ips[q])\n",
    "        _MD = _MD / k\n",
    "        MD[i] = _MD\n",
    "    return MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_WMTGS(X, y):\n",
    "    \"\"\"\n",
    "    input: X, y\n",
    "    output: reduced_model_scaler, feature_weight, select_columns\n",
    "\n",
    "    reduced_model_scaler: 縮小モデルのスケーラー\n",
    "    feature_weight: 縮小モデルの特徴量の重み\n",
    "    select_columns: 選択された変数\n",
    "    \"\"\"\n",
    "    # 正常データのみを使用して標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[y == 0])\n",
    "    normal_Z = scaler.transform(X[y == 0])\n",
    "    anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "    # 異常データと直交表を用いてSN比を計算\n",
    "    #L8直行表\n",
    "    l8 = np.array([\n",
    "        [1,1,1,1,1,1,1],\n",
    "        [1,1,1,2,2,2,2],\n",
    "        [1,2,2,1,1,2,2],\n",
    "        [1,2,2,2,2,1,1],\n",
    "        [2,1,2,1,2,1,2],\n",
    "        [2,1,2,2,1,2,1],\n",
    "        [2,2,1,1,2,2,1],\n",
    "        [2,2,1,2,1,1,2]\n",
    "        ])\n",
    "    l8 = (l8 == 1)\n",
    "\n",
    "    # 異常データのマハラノビス距離\n",
    "    anomaly_MD = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "    for i, l8_row in enumerate(l8):\n",
    "        anomaly_MD[i] = gram_schmidt_cal_MD(anomaly_Z[:, l8_row]) # 直交化のタイミングは？これだと異常データを直交化してる\n",
    "        # おそらくUでSN比を計算する\n",
    "    print(anomaly_MD)\n",
    "\n",
    "    # SN比の算出\n",
    "    sn = np.zeros(l8.shape[0])\n",
    "    for idx, row in enumerate(anomaly_MD):\n",
    "        sum_MD = 0\n",
    "        for row_i in row:\n",
    "            sum_MD += 1 / row_i\n",
    "        sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "        \n",
    "    # SN比を利用し，不要と思われる変数を削除する\n",
    "    # 変数選択\n",
    "    df_gain = pd.DataFrame(index=X.columns, columns=['効果ゲイン'])\n",
    "    for i, clm in enumerate(X.columns):\n",
    "        gain = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "        df_gain.loc[df_gain.index == clm, '効果ゲイン'] = gain\n",
    "    # 選択された変数を保存\n",
    "    select_columns = df_gain[df_gain['効果ゲイン'] > 0].index\n",
    "    \n",
    "    \n",
    "    # 選択された変数が1つ以下の場合の例外処理\n",
    "    if len(select_columns) > 1:\n",
    "        # 縮小モデルでのスケーラーと共分散行列を計算\n",
    "        reduced_model_scaler = StandardScaler()\n",
    "        reduced_model_scaler.fit(X[select_columns][y == 0])\n",
    "        # 重みを保存\n",
    "        feature_weight = (df_gain.loc[select_columns] / df_gain.loc[select_columns].sum()).values\n",
    "    # 選択された変数が一つ以下の場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        select_columns = df_gain['効果ゲイン'].astype(float).idxmax()\n",
    "        reduced_model_scaler = X[select_columns][y == 0].mean()\n",
    "        feature_weight = X[select_columns][y == 0].std()\n",
    "\n",
    "\n",
    "    return reduced_model_scaler, feature_weight, select_columns\n",
    "\n",
    "# 縮小モデルによってマハラノビス距離を計算する\n",
    "def gram_schmidt_cal_MD_by_reduced_model(X, reduced_model_scaler, feature_weight, select_columns):\n",
    "    # select_columnsがfloatになることがある？\n",
    "    if type(reduced_model_scaler) == StandardScaler:\n",
    "        Z = reduced_model_scaler.transform(X[select_columns])\n",
    "        MD = gram_schmidt_cal_MD(Z, feature_weight)\n",
    "    # 変数が一つしか選択されなかった場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        MD = ((X[select_columns] - reduced_model_scaler) / feature_weight) ** 2\n",
    "    return MD\n",
    "\n",
    "def predict_WMTGS(X_test, reduced_model_scaler, feature_weight, select_columns, threshold):\n",
    "    proba = gram_schmidt_cal_MD_by_reduced_model(X_test, reduced_model_scaler, feature_weight, select_columns)\n",
    "    pred = proba > threshold\n",
    "    return proba, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55374356 0.86808388 1.18225135 ... 1.04045303 1.46085567 0.53424671]\n",
      " [0.60913342 0.65978089 0.57324622 ... 1.4859584  1.77811602 0.25698666]\n",
      " [0.92303311 1.17351489 1.7786189  ... 1.04109025 1.19587482 0.73040287]\n",
      " ...\n",
      " [0.37729101 0.67742734 0.63723131 ... 0.75152921 1.5678866  0.10127744]\n",
      " [0.80576905 1.74328186 1.29606174 ... 1.318261   0.60235032 1.19467536]\n",
      " [1.00378495 1.47406687 0.60598906 ... 1.62831045 1.8827214  1.46878261]]\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, feature_weight, select_columns = fit_WMTGS(X_train, y_train)\n",
    "y_proba_train = gram_schmidt_cal_MD_by_reduced_model(X_train, reduced_model_scaler, feature_weight, select_columns)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_WMTGS(X_test, reduced_model_scaler, feature_weight, select_columns, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46388977827583444\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55374356 0.86808388 1.18225135 ... 1.04045303 1.46085567 0.53424671]\n",
      " [0.60913342 0.65978089 0.57324622 ... 1.4859584  1.77811602 0.25698666]\n",
      " [0.92303311 1.17351489 1.7786189  ... 1.04109025 1.19587482 0.73040287]\n",
      " ...\n",
      " [0.37729101 0.67742734 0.63723131 ... 0.75152921 1.5678866  0.10127744]\n",
      " [0.80576905 1.74328186 1.29606174 ... 1.318261   0.60235032 1.19467536]\n",
      " [1.00378495 1.47406687 0.60598906 ... 1.62831045 1.8827214  1.46878261]]\n",
      "0.46388977827583444\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, feature_weight, select_columns = fit_WMTGS(X_train, y_train)\n",
    "y_proba_train = gram_schmidt_cal_MD_by_reduced_model(X_train, reduced_model_scaler, feature_weight, select_columns)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_WMTGS(X_test, reduced_model_scaler, feature_weight, select_columns, threshold)\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.43353511  3.45841872  4.66272388 ...  3.99618571  6.966797\n",
      "   3.37775447]\n",
      " [ 3.34211066  2.34920689  1.67698028 ... 10.75428927  6.71097596\n",
      "   1.86915639]\n",
      " [ 2.55257625  3.42176251  3.66494702 ...  1.70346274  2.33673349\n",
      "   1.40839504]\n",
      " ...\n",
      " [ 1.1337538   4.22292582  1.69113612 ...  4.1120787   4.59842975\n",
      "   0.48379127]\n",
      " [ 2.54233103 10.65263276  3.50780256 ... 11.3289156   1.28692469\n",
      "   3.98756192]\n",
      " [ 3.72405432  8.91262925  3.56644918 ...  9.98440544  8.39368719\n",
      "   8.1239551 ]]\n",
      "0.884659576029243\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, reduced_model_inv_C, select_columns = fit_MTS(X_train, y_train)\n",
    "y_proba_train = cal_MD_by_reduced_model(X_train, reduced_model_scaler, reduced_model_inv_C, select_columns)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_MTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, threshold)\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題！！！\n",
    "- 異常データのマハラノビス距離を求める際に，MTSでは正常データの標準偏差，平均で標準化した後，正常データの分散共分散行列でマハラノビス距離を求める．つまり，異常データを正常データと同じ軸で扱っている．\n",
    "- しかし，グラムシュミット法では，異常データで直交化を行い，さらに共分散行列を用いないため，異常データのイプシロンでマハラノビス距離を求めている．\n",
    "- 正常データでマハラノビス距離を求めるときは結果は一緒になるが異常データを用いるとマハラノビス距離がやや変わってしまう？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTGSでは直交表使わないっぽいぞ？？？？？\n",
    "### いや使うけど計算方法が違うっぽい！\n",
    "### MTGSでは変数ごとにMDが求められるから直交化は全部の変数でやってMDの計算のところだけ変数を入れるか入れないか区別する！！！！\n",
    "#### まあ現状のでも悪くないかもだけど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20220413\n",
    "### とりあえずできそうなことやってみる．金曜日にわからないことまとめて月曜日に先生に質問投げる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7e6621f5c0e725993c5f5dd1734f3da8dc8c958ed2c46496e37b878d46070df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('convenient')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
