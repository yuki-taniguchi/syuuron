{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSのZにSN比の効果ゲインを重みづけを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.dates as mdates\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "#Aのみを判定するため，Aを１，A以外を0にした．\n",
    "df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "#Xとyを入力\n",
    "X = df[range(1,17)]\n",
    "y = df[0]\n",
    "\n",
    "#バギング側の話\n",
    "#ブートストラップサンプリングの個数\n",
    "n = 10\n",
    "seed = random.randint(0, n)\n",
    "\n",
    "#使用する7つの変数をランダムに取得する\n",
    "random.seed(6)\n",
    "random_s = random.sample(list(X.columns), 7)\n",
    "use_X = X[random_s]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(use_X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な関数の定義\n",
    "\n",
    "# 共分散行列の逆行列\n",
    "def inv_cov(Z):\n",
    "    #標準化後のベクトルを入力する\n",
    "    #標準化した後なので相関行列と分散共分散行列は一致する\n",
    "    c = np.cov(Z.T)\n",
    "    return np.linalg.pinv(c)\n",
    "\n",
    "#マハラノビス汎距離\n",
    "def cal_MD(Z, inv_C):\n",
    "    '''\n",
    "    Z:標準化したベクトル\n",
    "    inv_C:分散共分散行列の逆行列\n",
    "    '''\n",
    "    MD = np.zeros(len(Z))\n",
    "    for i in range(len(Z)):\n",
    "        _a = np.dot(Z[i], inv_C)\n",
    "        _MD = np.dot(_a, Z[i].T)\n",
    "        _MD = _MD / Z.shape[1]\n",
    "        MD[i] = _MD\n",
    "    return MD\n",
    "    \n",
    "# 閾値をジニ係数が最小になるように決定する\n",
    "def determine_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        y_true: trainデータのラベルを入力\n",
    "        y_proba: trainデータの異常度（縮小モデルのMD）を入力\n",
    "    output: threshold\n",
    "    \"\"\"\n",
    "    df_ = pd.DataFrame(y_true)\n",
    "    df_['proba'] = y_proba\n",
    "    df_ = df_.sort_values('proba').reset_index(drop=True)\n",
    "\n",
    "    min_gini = np.inf\n",
    "    threshold = 0\n",
    "    for i in range(len(df_)):\n",
    "        \n",
    "        neg = df_.iloc[:i+1]\n",
    "        pos = df_.iloc[i:]\n",
    "\n",
    "        p_neg = sum(neg[y_true.name]) / len(neg)\n",
    "        gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "        p_pos = sum(pos[y_true.name]) / len(pos)\n",
    "        gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "        gini_split = (len(neg) / len(df_) * gini_neg) + (len(pos) / len(df_) * gini_pos)\n",
    "\n",
    "        if min_gini > gini_split:\n",
    "            min_gini = gini_split\n",
    "            threshold = df_.iloc[i]['proba']\n",
    "            threshold_idx = i\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みづけなしのMTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTSを実行\n",
    "def fit_MTS(X, y):\n",
    "    \"\"\"\n",
    "    input: X, y\n",
    "    output: reduced_model_scaler, reduced_model_inv_C, select_columns\n",
    "\n",
    "    reduced_model_scaler: 縮小モデルのスケーラー\n",
    "    reduced_model_inv_C: 縮小モデルの共分散行列の逆行列\n",
    "    select_columns: 選択された変数\n",
    "    \"\"\"\n",
    "    # 正常データのみを使用して標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[y == 0])\n",
    "    normal_Z = scaler.transform(X[y == 0])\n",
    "    anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "    # 正常データのみを使用して共分散行列を計算\n",
    "    inv_C = inv_cov(normal_Z)\n",
    "\n",
    "    # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "        #１度目の仮のマハラノビス距離を計算\n",
    "        # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "        # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "        # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "    # 異常データと直交表を用いてSN比を計算\n",
    "    #L8直行表\n",
    "    l8 = np.array([\n",
    "        [1,1,1,1,1,1,1],\n",
    "        [1,1,1,2,2,2,2],\n",
    "        [1,2,2,1,1,2,2],\n",
    "        [1,2,2,2,2,1,1],\n",
    "        [2,1,2,1,2,1,2],\n",
    "        [2,1,2,2,1,2,1],\n",
    "        [2,2,1,1,2,2,1],\n",
    "        [2,2,1,2,1,1,2]\n",
    "        ])\n",
    "    l8 = (l8 == 1)\n",
    "\n",
    "    # 異常データのマハラノビス距離\n",
    "    anomaly_MD = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "    for i, l8_row in enumerate(l8):\n",
    "        anomaly_MD[i] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row]) # 正常データのinv_Cを使う必要がある\n",
    "\n",
    "    # SN比の算出\n",
    "    sn = np.zeros(l8.shape[0])\n",
    "    for idx, row in enumerate(anomaly_MD):\n",
    "        sum_MD = 0\n",
    "        for row_i in row:\n",
    "            sum_MD += 1 / row_i\n",
    "        sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "        \n",
    "    # SN比を利用し，不要と思われる変数を削除する\n",
    "    # 変数選択\n",
    "    df_gain = pd.DataFrame(index=X.columns, columns=['効果ゲイン','残す'])\n",
    "    for i, clm in enumerate(X.columns):\n",
    "        gain = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "        df_gain.loc[df_gain.index == clm, '効果ゲイン'] = gain\n",
    "        df_gain.loc[df_gain.index == clm, '残す'] = gain > 0\n",
    "    # 選択された変数を保存\n",
    "    select_columns = df_gain[df_gain['残す']].index\n",
    "    \n",
    "    # 選択された変数が1つ以下の場合の例外処理\n",
    "    if len(select_columns) > 1:\n",
    "        # 縮小モデルでのスケーラーと共分散行列を計算\n",
    "        reduced_model_scaler = StandardScaler()\n",
    "        reduced_model_scaler.fit(X[select_columns][y == 0])\n",
    "        reduced_model_normal_Z = reduced_model_scaler.transform(X[select_columns][y == 0])\n",
    "        reduced_model_inv_C = inv_cov(reduced_model_normal_Z)\n",
    "    # 選択された変数が一つ以下の場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        select_columns = df_gain['効果ゲイン'].astype(float).idxmax()\n",
    "        reduced_model_scaler = X[select_columns][y == 0].mean()\n",
    "        reduced_model_inv_C = X[select_columns][y == 0].std()\n",
    "\n",
    "    # 縮小モデルのスケーラーと共分散行列と選択した変数を出力\n",
    "    return reduced_model_scaler, reduced_model_inv_C, select_columns\n",
    "# 縮小モデルによってマハラノビス距離を計算する\n",
    "def cal_MD_by_reduced_model(X, reduced_model_scaler, reduced_model_inv_C, select_columns):\n",
    "    # select_columnsがfloatになることがある？\n",
    "    if type(reduced_model_scaler) == StandardScaler:\n",
    "        Z = reduced_model_scaler.transform(X[select_columns])\n",
    "        MD = cal_MD(Z, reduced_model_inv_C)\n",
    "    # 変数が一つしか選択されなかった場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        MD = ((X[select_columns] - reduced_model_scaler) / reduced_model_inv_C) ** 2\n",
    "    return MD\n",
    "def predict_MTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, threshold):\n",
    "    proba = cal_MD_by_reduced_model(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns)\n",
    "    pred = proba > threshold\n",
    "    return proba, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みづけMTS\n",
    "1. 単位空間の作成\n",
    "    1. Xを正常データのみで標準化してZを取得\n",
    "    2. 正常データのみで共分散行列の逆行列Inv_Cを取得\n",
    "2. 縮小単位空間作成のための変数選択\n",
    "    1. 異常データ（anomaly_Z）を用いてSN比を算出\n",
    "    2. 直交表(現状はL8のみ)を用いて各変数の効果ゲインを算出\n",
    "    3. 効果ゲインが負の変数を削除し，縮小単位空間を作成\n",
    "    4. <u>**縮小単位空間において効果ゲインを正規化したものをその変数の重みとして保存**</u>\n",
    "3. 縮小単位空間の閾値決定\n",
    "    1. gini係数が最小となる閾値を算出\n",
    "4. 新しいデータの予測\n",
    "    1. 縮小単位空間によって新しいデータのマハラノビス距離を算出し，異常度とする．\n",
    "    2. その異常度が閾値を超えたら異常と予測する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTSを実行\n",
    "def fit_WMTS(X, y):\n",
    "    \"\"\"\n",
    "    input: X, y\n",
    "    output: reduced_model_scaler, reduced_model_inv_C, select_columns\n",
    "\n",
    "    reduced_model_scaler: 縮小モデルのスケーラー\n",
    "    reduced_model_inv_C: 縮小モデルの共分散行列の逆行列\n",
    "    select_columns: 選択された変数\n",
    "    \"\"\"\n",
    "    # 正常データのみを使用して標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[y == 0])\n",
    "    normal_Z = scaler.transform(X[y == 0])\n",
    "    anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "    # 正常データのみを使用して共分散行列を計算\n",
    "    inv_C = inv_cov(normal_Z)\n",
    "\n",
    "    # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "        #１度目の仮のマハラノビス距離を計算\n",
    "        # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "        # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "        # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "    # 異常データと直交表を用いてSN比を計算\n",
    "    #L8直行表\n",
    "    l8 = np.array([\n",
    "        [1,1,1,1,1,1,1],\n",
    "        [1,1,1,2,2,2,2],\n",
    "        [1,2,2,1,1,2,2],\n",
    "        [1,2,2,2,2,1,1],\n",
    "        [2,1,2,1,2,1,2],\n",
    "        [2,1,2,2,1,2,1],\n",
    "        [2,2,1,1,2,2,1],\n",
    "        [2,2,1,2,1,1,2]\n",
    "        ])\n",
    "    l8 = (l8 == 1)\n",
    "\n",
    "    # 異常データのマハラノビス距離\n",
    "    anomaly_MD = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "    for i, l8_row in enumerate(l8):\n",
    "        anomaly_MD[i] = cal_MD(anomaly_Z[:, l8_row], inv_C[l8_row][:,l8_row]) # 正常データのinv_Cを使う必要がある\n",
    "\n",
    "    # SN比の算出\n",
    "    sn = np.zeros(l8.shape[0])\n",
    "    for idx, row in enumerate(anomaly_MD):\n",
    "        sum_MD = 0\n",
    "        for row_i in row:\n",
    "            sum_MD += 1 / row_i\n",
    "        sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "        \n",
    "    # SN比を利用し，不要と思われる変数を削除する\n",
    "    # 変数選択\n",
    "    df_gain = pd.DataFrame(index=X.columns, columns=['効果ゲイン','残す'])\n",
    "    for i, clm in enumerate(X.columns):\n",
    "        gain = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "        df_gain.loc[df_gain.index == clm, '効果ゲイン'] = gain\n",
    "        df_gain.loc[df_gain.index == clm, '残す'] = gain > 0\n",
    "    # 選択された変数を保存\n",
    "    select_columns = df_gain[df_gain['残す']].index\n",
    "    select_gain = df_gain[df_gain['残す']]['効果ゲイン'].values\n",
    "    select_columns_weight = select_gain / select_gain.sum()\n",
    "    \n",
    "    # 選択された変数が1つ以下の場合の例外処理\n",
    "    if len(select_columns) > 1:\n",
    "        # 縮小モデルでのスケーラーと共分散行列を計算\n",
    "        reduced_model_scaler = StandardScaler()\n",
    "        reduced_model_scaler.fit(X[select_columns][y == 0])\n",
    "        reduced_model_normal_Z = reduced_model_scaler.transform(X[select_columns][y == 0])\n",
    "        reduced_model_inv_C = inv_cov(reduced_model_normal_Z)\n",
    "    # 選択された変数が一つ以下の場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        select_columns = df_gain['効果ゲイン'].astype(float).idxmax()\n",
    "        reduced_model_scaler = X[select_columns][y == 0].mean()\n",
    "        reduced_model_inv_C = X[select_columns][y == 0].std()\n",
    "\n",
    "    # 縮小モデルのスケーラーと共分散行列と選択した変数を出力\n",
    "    return reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight\n",
    "# 縮小モデルによってマハラノビス距離を計算する\n",
    "def cal_WMD_by_reduced_model(X, reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight):\n",
    "    # select_columnsがfloatになることがある？\n",
    "    if type(reduced_model_scaler) == StandardScaler:\n",
    "        Z = reduced_model_scaler.transform(X[select_columns])\n",
    "        Weighted_Z = Z * select_columns_weight\n",
    "        MD = cal_MD(Weighted_Z, reduced_model_inv_C)\n",
    "    # 変数が一つしか選択されなかった場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "    else:\n",
    "        MD = ((X[select_columns] - reduced_model_scaler) / reduced_model_inv_C) ** 2\n",
    "    return MD\n",
    "def predict_WMTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight, threshold):\n",
    "    proba = cal_WMD_by_reduced_model(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight)\n",
    "    pred = proba > threshold\n",
    "    return proba, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/letter_recognition.csv', header=None)\n",
    "\n",
    "#Aのみを判定するため，Aを１，A以外を0にした．\n",
    "df[0] = df[0].apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "#Xとyを入力\n",
    "X = df[range(1,17)]\n",
    "y = df[0]\n",
    "\n",
    "#バギング側の話\n",
    "#ブートストラップサンプリングの個数\n",
    "n = 10\n",
    "seed = random.randint(0, n)\n",
    "\n",
    "#使用する7つの変数をランダムに取得する\n",
    "random.seed(7)\n",
    "random_s = random.sample(list(X.columns), 7)\n",
    "use_X = X[random_s]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(use_X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501359307359307\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight = fit_WMTS(X_train, y_train)\n",
    "y_proba_train = cal_WMD_by_reduced_model(X_train, reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_WMTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, select_columns_weight, threshold)\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9499021645021645\n"
     ]
    }
   ],
   "source": [
    "reduced_model_scaler, reduced_model_inv_C, select_columns = fit_MTS(X_train, y_train)\n",
    "y_proba_train = cal_MD_by_reduced_model(X_train, reduced_model_scaler, reduced_model_inv_C, select_columns)\n",
    "threshold = determine_threshold(y_train, y_proba_train)\n",
    "proba, pred = predict_MTS(X_test, reduced_model_scaler, reduced_model_inv_C, select_columns, threshold)\n",
    "print(roc_auc_score(y_test.values, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スコアが良くなる時もある！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7e6621f5c0e725993c5f5dd1734f3da8dc8c958ed2c46496e37b878d46070df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('convenient')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
