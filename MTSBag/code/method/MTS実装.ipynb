{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../func\")\n",
    "from my_func import inv_cov, cal_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTSClassifier():\n",
    "    \"\"\"\n",
    "    MTS 分類器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, add_weight=False):\n",
    "\n",
    "        self.add_weight = add_weight\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 正常データのみを使用して標準化\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X[y == 0])\n",
    "        normal_Z = scaler.transform(X[y == 0])\n",
    "        anomaly_Z = scaler.transform(X[y == 1])\n",
    "\n",
    "        # 正常データのみを使用して共分散行列を計算\n",
    "        self.inv_C = inv_cov(normal_Z)\n",
    "\n",
    "        # いったん飛ばす，削除の基準は？削除しない方法もあるっぽい？\n",
    "            #１度目の仮のマハラノビス距離を計算\n",
    "            # MD_1st = cal_MD(normal_Z, inv_C)\n",
    "            # もしもマハラノビス距離が余りにも大きいサンプルがあれば任意で削除する\n",
    "            # 削除後のデータを使用して標準化と共分散行列を計算\n",
    "\n",
    "        # 異常データと直交表を用いてSN比を計算\n",
    "        #L8直行表\n",
    "        l8 = np.array([\n",
    "            [1,1,1,1,1,1,1],\n",
    "            [1,1,1,2,2,2,2],\n",
    "            [1,2,2,1,1,2,2],\n",
    "            [1,2,2,2,2,1,1],\n",
    "            [2,1,2,1,2,1,2],\n",
    "            [2,1,2,2,1,2,1],\n",
    "            [2,2,1,1,2,2,1],\n",
    "            [2,2,1,2,1,1,2]\n",
    "            ])\n",
    "        l8 = (l8 == 1)\n",
    "\n",
    "        # 異常データのマハラノビス距離\n",
    "        anomaly_MD = np.zeros((l8.shape[0], anomaly_Z.shape[0]))\n",
    "        for i, l8_row in enumerate(l8):\n",
    "            anomaly_MD[i] = cal_MD(anomaly_Z[:, l8_row], self.inv_C[l8_row][:,l8_row]) # 正常データのinv_Cを使う必要がある\n",
    "\n",
    "        # SN比の算出\n",
    "        sn = np.zeros(l8.shape[0])\n",
    "        for idx, row in enumerate(anomaly_MD):\n",
    "            sum_MD = 0\n",
    "            for row_i in row:\n",
    "                sum_MD += 1 / row_i\n",
    "            sn[idx] = -10 * math.log10(sum_MD / len(row))\n",
    "            \n",
    "        # SN比を利用し，不要と思われる変数を削除する\n",
    "        # 変数選択\n",
    "        df_gain = pd.DataFrame(index=X.columns, columns=['効果ゲイン','残す'])\n",
    "        for i, clm in enumerate(X.columns):\n",
    "            gain = sum(sn[l8.T[i]]) - sum(sn[~l8.T[i]])\n",
    "            df_gain.loc[df_gain.index == clm, '効果ゲイン'] = gain\n",
    "            df_gain.loc[df_gain.index == clm, '残す'] = gain > 0\n",
    "        # 選択された変数を保存\n",
    "        self.select_columns = df_gain[df_gain['残す']].index\n",
    "        \n",
    "        # 選択された変数が1つ以下の場合の例外処理\n",
    "        if len(self.select_columns) > 1:\n",
    "            select_gain = df_gain[df_gain['残す']]['効果ゲイン'].values\n",
    "            self.select_columns_weight = select_gain / select_gain.sum()\n",
    "            # 縮小モデルでのスケーラーと共分散行列を計算\n",
    "            self.reduced_model_scaler = StandardScaler()\n",
    "            self.reduced_model_scaler.fit(X[self.select_columns][y == 0])\n",
    "            self.reduced_model_normal_Z = self.reduced_model_scaler.transform(X[self.select_columns][y == 0])\n",
    "            self.reduced_model_inv_C = inv_cov(self.reduced_model_normal_Z)\n",
    "        # 選択された変数が一つ以下の場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "        else:\n",
    "            self.select_columns = df_gain['効果ゲイン'].astype(float).idxmax()\n",
    "            self.reduced_model_mean = X[self.select_columns][y == 0].mean()\n",
    "            self.reduced_model_std = X[self.select_columns][y == 0].std()\n",
    "            self.select_columns_weight = 1\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # select_columnsがfloatになることがある？\n",
    "        if type(self.reduced_model_scaler) == StandardScaler: # 変更したほうがいいかも？\n",
    "            Z = self.reduced_model_scaler.transform(X[self.select_columns])\n",
    "            if self.add_weight:\n",
    "                Weighted_Z = Z * self.select_columns_weight\n",
    "                MD = cal_MD(Weighted_Z, self.reduced_model_inv_C)\n",
    "            else:\n",
    "                MD = cal_MD(Z, self.reduced_model_inv_C)\n",
    "        # 変数が一つしか選択されなかった場合はその変数を正常データの平均と標準偏差で標準化してそれの二乗を異常値とする\n",
    "        else:\n",
    "            MD = ((X[self.select_columns] - self.reduced_model_mean) / self.reduced_model_std) ** 2\n",
    "        return MD\n",
    "\n",
    "    def determine_threshold(self, X, y):\n",
    "        proba = self.predict_proba(X)\n",
    "        _df = pd.DataFrame(y)\n",
    "        _df['proba'] = proba\n",
    "        _df = _df.sort_values('proba').reset_index(drop=True)\n",
    "\n",
    "        min_gini = np.inf\n",
    "        self.threshold = 0\n",
    "        for i in range(len(_df)):\n",
    "            \n",
    "            neg = _df.iloc[:i+1]\n",
    "            pos = _df.iloc[i:]\n",
    "\n",
    "            p_neg = sum(neg[y.name]) / len(neg)\n",
    "            gini_neg = 1 - ( p_neg ** 2 + ( 1 - p_neg ) ** 2 )\n",
    "\n",
    "            p_pos = sum(pos[y.name]) / len(pos)\n",
    "            gini_pos = 1 - ( p_pos ** 2 + ( 1 - p_pos ) ** 2 )\n",
    "\n",
    "            gini_split = (len(neg) / len(_df) * gini_neg) + (len(pos) / len(_df) * gini_pos)\n",
    "\n",
    "            if min_gini > gini_split:\n",
    "                min_gini = gini_split\n",
    "                self.threshold = _df.iloc[i]['proba']\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        determine_thresholdを実行してから実行する\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        \n",
    "        return proba > self.threshold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "# ファイルのダウンロード\n",
    "dataset_path = tf.keras.utils.get_file('wine.data', dataset_url)\n",
    "\n",
    "# print(dataset_path)\n",
    "\n",
    "raw_data = pd.read_csv(dataset_path, names=range(13))\n",
    "raw_data['y'] = raw_data.index\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "raw_data['y'] = raw_data['y'].apply(lambda x: 0 if x == 3 else 1)\n",
    "\n",
    "X = raw_data.drop('y', axis=1)\n",
    "y = raw_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_s = random.sample(list(X.columns), len(X.columns) if len(X.columns) < 7 else 7)\n",
    "X = X[random_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9961538461538462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [ 2, 24]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MTSClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)))\n",
    "clf.determine_threshold(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [ 2, 24]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MTSClassifier(add_weight=True)\n",
    "clf.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)))\n",
    "clf.determine_threshold(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7e6621f5c0e725993c5f5dd1734f3da8dc8c958ed2c46496e37b878d46070df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('convenient')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
